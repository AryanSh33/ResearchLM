{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCHNx2L0rbH9"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time  # for rate-limiting if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzOW--BCveQ2"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqawh1WT9ZHy"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "API_KEY = \"Vs5X8oxztw9W8Uc5OUWEP8eKQgi3apKcaHxVy63a\"\n",
        "SEARCH_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "DETAILS_URL = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"x-api-key\": API_KEY\n",
        "}\n",
        "\n",
        "def search_papers(query, limit=20):\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"fields\": \"title,abstract,year,citationCount,authors,url\",\n",
        "        \"limit\": limit,\n",
        "        \"offset\": 0\n",
        "    }\n",
        "    response = requests.get(SEARCH_URL, headers=HEADERS, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"data\", [])\n",
        "    else:\n",
        "        print(\"Search Error:\", response.status_code, response.text)\n",
        "        return []\n",
        "\n",
        "def rank_papers(papers):\n",
        "    # Rank by: citations * recency_weight\n",
        "    current_year = 2025\n",
        "    for paper in papers:\n",
        "        year = paper.get(\"year\", 2000)\n",
        "        citations = paper.get(\"citationCount\", 0)\n",
        "        recency_weight = 1 + (year - 2000) / 25  # More recent gets slight boost\n",
        "        paper[\"score\"] = citations * recency_weight\n",
        "    return sorted(papers, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "def retrieve_top_papers(query, top_k=5):\n",
        "    collected = []\n",
        "    offset = 0\n",
        "    limit = 20\n",
        "    max_pages = 10  # Avoid infinite loops\n",
        "\n",
        "    while len(collected) < top_k and max_pages > 0:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"fields\": \"title,abstract,year,citationCount,authors,url\",\n",
        "            \"limit\": limit,\n",
        "            \"offset\": offset\n",
        "        }\n",
        "        response = requests.get(SEARCH_URL, headers=HEADERS, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Search Error:\", response.status_code, response.text)\n",
        "            break\n",
        "\n",
        "        papers = response.json().get(\"data\", [])\n",
        "        if not papers:\n",
        "            break\n",
        "\n",
        "        for paper in papers:\n",
        "            if paper.get(\"abstract\"):  # Only include papers with valid abstract\n",
        "                collected.append(paper)\n",
        "                if len(collected) == top_k:\n",
        "                    break\n",
        "\n",
        "        offset += limit\n",
        "        max_pages -= 1\n",
        "\n",
        "    if len(collected) < top_k:\n",
        "        print(f\"⚠️ Only {len(collected)} papers found with abstracts.\")\n",
        "\n",
        "    # Rank the filtered papers\n",
        "    ranked = rank_papers(collected)\n",
        "    top_papers = ranked[:top_k]\n",
        "\n",
        "    # Return dictionary format\n",
        "    results = {}\n",
        "    for idx, paper in enumerate(top_papers, 1):\n",
        "        results[f\"Paper {idx}\"] = {\n",
        "            \"title\": paper[\"title\"],\n",
        "            \"abstract\": paper[\"abstract\"],\n",
        "            \"year\": paper[\"year\"],\n",
        "            \"citations\": paper[\"citationCount\"],\n",
        "            \"url\": paper.get(\"url\", \"\")\n",
        "        }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGN_3SmK_9qF",
        "outputId": "8e6b7cc3-943d-4dd7-e6c2-3961d87f8fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Results saved to top_climate_papers.json in dictionary format.\n"
          ]
        }
      ],
      "source": [
        "# ------------------ EXAMPLE USAGE ------------------ #\n",
        "if __name__ == \"__main__\":\n",
        "    import json\n",
        "\n",
        "    user_query = \"Brain Tumor Segmentation using Machine Learning\"\n",
        "    results_dict = retrieve_top_papers(user_query)\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open(\"ml.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results_dict, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"📄 Results saved to top_climate_papers.json in dictionary format.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcFOweZQETaS"
      },
      "source": [
        "**Now, we have extracted the data (in json format) from one API, and then sending it into another API to do further analysis on the retrieved research Papers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3xTJ9amrhAJ"
      },
      "outputs": [],
      "source": [
        "# Configure Gemini\n",
        "genai.configure(api_key='AIzaSyDBCwgbIw4q1jqIbVfIA1Ax4Gl_O0vAbYY')\n",
        "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ69q1MjC7yQ"
      },
      "outputs": [],
      "source": [
        "GEMINI_URL = \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=\" + \"AIzaSyDBCwgbIw4q1jqIbVfIA1Ax4Gl_O0vAbYY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeBWFqKFsFrI"
      },
      "outputs": [],
      "source": [
        "# # 🧠 Prompt Template\n",
        "# def make_prompt(abstract):\n",
        "#     return f\"\"\"\n",
        "# You are a research assistant performing a literature survey.\n",
        "\n",
        "# Given the abstract of a research paper, respond exactly with the following fields in plain text:\n",
        "# Paper Title: ...\n",
        "# Summary: ...\n",
        "# Challenges: ...\n",
        "# Scope for Improvement: ...\n",
        "# (Tip: If any of the following sections is not properly mentioned, could u please try to insert it based on the context.)\n",
        "# ### Abstract:\n",
        "# {abstract}\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBKd2Go7sw_Z"
      },
      "outputs": [],
      "source": [
        "def analyze_abstract(json_path):\n",
        "    # Load abstract data\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        papers = json.load(f)\n",
        "\n",
        "    analyzed_results = {}\n",
        "\n",
        "    for paper_id, paper in papers.items():\n",
        "        abstract = paper.get(\"abstract\", \"\")\n",
        "        if not abstract:\n",
        "            continue\n",
        "\n",
        "        prompt = f\"\"\"You are a research assistant. Analyze the following abstract and provide:\n",
        "                      1. A brief summary\n",
        "                      2. Key challenges mentioned or implied\n",
        "                      3. Scope for improvement or future work\n",
        "                (Tip: If any of the following sections is not properly mentioned, could u please try to insert it based on the context.)\n",
        "Abstract:\n",
        "\\\"\\\"\\\"\n",
        "{abstract}\n",
        "\\\"\\\"\\\"\"\"\"\n",
        "\n",
        "        payload = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [{\"text\": prompt}]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        response = requests.post(GEMINI_URL, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            gemini_reply = response.json()\n",
        "            generated_text = gemini_reply[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "\n",
        "            analyzed_results[paper_id] = {\n",
        "                \"title\": paper[\"title\"],\n",
        "                \"year\": paper[\"year\"],\n",
        "                \"citations\": paper[\"citations\"],\n",
        "                \"url\": paper[\"url\"],\n",
        "                \"analysis\": generated_text.strip()\n",
        "            }\n",
        "            print(f\"✅ Processed {paper_id}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed {paper_id} - {response.status_code}\")\n",
        "            analyzed_results[paper_id] = {\n",
        "                \"title\": paper[\"title\"],\n",
        "                \"error\": response.text\n",
        "            }\n",
        "\n",
        "    return analyzed_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2LcRywlsx8m"
      },
      "outputs": [],
      "source": [
        "# # List of abstracts to analyze\n",
        "# abstracts = [\n",
        "#     \"\"\"One of the aspects of quantum theory which has attracted the most general attention, is the novelty of the logical notions which it presupposes. It asserts that even a complete mathematical description of a physical system S does not in general enable one to predict with certainty the result of an experiment on S, and that in particular one can never predict with certainty both the position and the momentum of S, (Heisenberg’s Uncertainty Principle). It further asserts that most pairs of observations are incompatible, and cannot be made on S, simultaneously (Principle of Non-commutativity of Observations).\"\"\",\n",
        "#     \"\"\" To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.\"\"\"\n",
        "#     \"\"\"Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of Images. Recently deep learning has been playing a major role in the field of computer vision. One of its applications is the reduction of human judgment in the diagnosis of diseases. Especially, brain tumor diagnosis requires high accuracy, where minute errors in judgment may lead to disaster. For this reason, brain tumor segmentation is an important challenge for medical purposes. Currently several methods exist for tumor segmentation but they all lack high accuracy. Here we present a solution for brain tumor segmenting by using deep learning. In this work, we studied different angles of brain MR images and applied different networks for segmentation. The effect of using separate networks for segmentation of MR images is evaluated by comparing the results with a single network. Experimental evaluations of the networks show that Dice score of 0.73 is achieved for a single network and 0.79 in obtained for multiple networks.\"\"\"\n",
        "#     # Add more abstracts here\n",
        "# ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XkpVZrOCXRl",
        "outputId": "8f49ccda-59b6-4676-d0b3-26d8bf0fab28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed Paper 1\n",
            "✅ Processed Paper 2\n",
            "✅ Processed Paper 3\n",
            "✅ Processed Paper 4\n",
            "✅ Processed Paper 5\n",
            "📊 Analysis saved to analyzed_climate_papers.json\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    results = analyze_abstract(\"top_climate_papers.json\")\n",
        "\n",
        "    with open(\"analyzed_climate_papers.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"📊 Analysis saved to analyzed_climate_papers.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "S7sgHTatvJLA",
        "outputId": "0cb81090-b1ef-4634-8b84-eac7a552af3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Processed Paper 1\n",
            "✅ Processed Paper 2\n",
            "📄 literature_survey.json created successfully!\n"
          ]
        }
      ],
      "source": [
        "# 🧾 Store all processed data (as dictionary) in json fromat\n",
        "results = {}\n",
        "\n",
        "for idx, abstract in enumerate(abstracts, start=1):\n",
        "    try:\n",
        "        data = analyze_abstract(abstract, idx)\n",
        "        results[f\"Paper {idx}\"] = data\n",
        "        print(f\"✅ Processed Paper {idx}\")\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing Paper {idx}: {e}\")\n",
        "\n",
        "# 💾 Save to JSON\n",
        "with open('literature_survey.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"📄 literature_survey.json created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCY3mj_LyWv3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8ZXBNlLHP-b"
      },
      "source": [
        "Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMwVVfktHfD2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuQIFT4810AX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "614df123-a528-4209-ff73-4c185e6a6a01"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://39d5dd88c2e97e0053.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://39d5dd88c2e97e0053.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# --- API KEYS ---\n",
        "SEMANTIC_SCHOLAR_API_KEY = \"Vs5X8oxztw9W8Uc5OUWEP8eKQgi3apKcaHxVy63a\"\n",
        "GEMINI_API_KEY = \"AIzaSyDBCwgbIw4q1jqIbVfIA1Ax4Gl_O0vAbYY\"\n",
        "\n",
        "# --- ENDPOINTS ---\n",
        "SEARCH_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "HEADERS = {\"x-api-key\": SEMANTIC_SCHOLAR_API_KEY}\n",
        "GEMINI_URL = f\"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "\n",
        "# STEP 1: Search and retrieve papers with a balance of citations and recency\n",
        "def retrieve_top_papers(query, top_k_cited=3, top_k_recent=2):\n",
        "    collected = []\n",
        "    offset = 0\n",
        "    limit = 20\n",
        "    max_pages = 10\n",
        "\n",
        "    while len(collected) < 100 and max_pages > 0:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"fields\": \"title,abstract,year,citationCount,authors,url\",\n",
        "            \"limit\": limit,\n",
        "            \"offset\": offset\n",
        "        }\n",
        "        response = requests.get(SEARCH_URL, headers=HEADERS, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Search Error:\", response.status_code)\n",
        "            break\n",
        "\n",
        "        papers = response.json().get(\"data\", [])\n",
        "        if not papers:\n",
        "            break\n",
        "\n",
        "        for paper in papers:\n",
        "            if paper.get(\"abstract\"):\n",
        "                collected.append(paper)\n",
        "\n",
        "        offset += limit\n",
        "        max_pages -= 1\n",
        "\n",
        "    current_year = 2025\n",
        "\n",
        "    # Top cited papers (irrespective of year)\n",
        "    cited_sorted = sorted(\n",
        "        collected, key=lambda x: x.get(\"citationCount\", 0), reverse=True)\n",
        "\n",
        "    # Recent and reasonably cited (e.g., year >= 2020 and citations >= 50)\n",
        "    recent_sorted = sorted(\n",
        "        [p for p in collected if p.get(\"year\", 0) >= current_year - 5 and p.get(\"citationCount\", 0) >= 50],\n",
        "        key=lambda x: (x.get(\"year\", 0), x.get(\"citationCount\", 0)),\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    final_papers = cited_sorted[:top_k_cited] + recent_sorted[:top_k_recent]\n",
        "\n",
        "    # Remove duplicates (same title)\n",
        "    seen_titles = set()\n",
        "    unique_final = []\n",
        "    for p in final_papers:\n",
        "        if p[\"title\"] not in seen_titles:\n",
        "            seen_titles.add(p[\"title\"])\n",
        "            unique_final.append(p)\n",
        "\n",
        "    result_dict = {}\n",
        "    for idx, paper in enumerate(unique_final, 1):\n",
        "        result_dict[f\"Paper {idx}\"] = {\n",
        "            \"title\": paper[\"title\"],\n",
        "            \"abstract\": paper[\"abstract\"],\n",
        "            \"year\": paper[\"year\"],\n",
        "            \"citations\": paper[\"citationCount\"],\n",
        "            \"url\": paper.get(\"url\", \"\")\n",
        "        }\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "# STEP 2: Analyze abstract using Gemini\n",
        "def analyze_abstract_dict(papers_dict):\n",
        "    analyzed = {}\n",
        "\n",
        "    for paper_id, paper in papers_dict.items():\n",
        "        abstract = paper[\"abstract\"]\n",
        "        prompt = f\"\"\"You are a research assistant. Analyze the following abstract and provide:\n",
        "1. A brief summary\n",
        "2. Key challenges mentioned or implied\n",
        "3. Authors' perspective\n",
        "4. Scope for improvement or future work\n",
        "(Tip: If any of the following sections is not properly mentioned, please infer and fill it based on context.)\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract}\\\"\\\"\\\"\"\"\"\n",
        "\n",
        "        payload = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [{\"text\": prompt}]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        response = requests.post(GEMINI_URL, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            gemini_reply = response.json()\n",
        "            generated_text = gemini_reply[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "            analyzed[paper_id] = {\n",
        "                \"title\": paper[\"title\"],\n",
        "                \"year\": paper[\"year\"],\n",
        "                \"citations\": paper[\"citations\"],\n",
        "                \"url\": paper[\"url\"],\n",
        "                \"analysis\": generated_text.strip()\n",
        "            }\n",
        "        else:\n",
        "            analyzed[paper_id] = {\n",
        "                \"title\": paper[\"title\"],\n",
        "                \"error\": f\"Error: {response.status_code}\"\n",
        "            }\n",
        "\n",
        "    return analyzed\n",
        "\n",
        "\n",
        "# STEP 3: Combined pipeline\n",
        "def literature_survey(query):\n",
        "    papers = retrieve_top_papers(query)\n",
        "    if not papers:\n",
        "        return \"❌ No papers found with abstracts for your query.\"\n",
        "\n",
        "    analysis = analyze_abstract_dict(papers)\n",
        "\n",
        "    output = \"\"\n",
        "    for pid, data in analysis.items():\n",
        "        output += f\"### {pid}: {data['title']}\\n\"\n",
        "        output += f\"- 📅 Year: {data.get('year', 'N/A')} | 📊 Citations: {data.get('citations', 'N/A')}\\n\"\n",
        "        output += f\"- 🔗 [Link to Paper]({data.get('url', '#')})\\n\"\n",
        "        output += f\"#### 🔍 Gemini Analysis:\\n{data.get('analysis', data.get('error', 'No analysis'))}\\n\\n\"\n",
        "        output += \"---\\n\"\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "# STEP 4: Launch Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=literature_survey,\n",
        "    inputs=gr.Textbox(placeholder=\"Enter a Research Topic:- \"),\n",
        "    outputs=gr.Markdown(),\n",
        "    title=\"📚 AI-Powered Literature Survey Assistant\",\n",
        "    description=\"Enter a research query. This app retrieves top foundational and recent relevant papers with abstracts, and uses Gemini AI to analyze them.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(debug = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2s6yHg32ZKB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mka_bJ8bgoQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}